{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uWHoHyJe1z_f",
    "outputId": "e9fa4699-12d7-45d9-b470-37a5c589c0e6"
   },
   "outputs": [],
   "source": [
    "# A dependency of the preprocessing for BERT inputs\n",
    "!pip install -q -U \"tensorflow-text==2.8.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4Z22ah5U15t7"
   },
   "outputs": [],
   "source": [
    "!pip install -q tf-models-official==2.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xiBJlkt917tA",
    "outputId": "5171dce3-1f62-4349-f077-3183d856304e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-05 17:04:29.742217: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-05 17:04:29.742262: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POt27IsPmUid"
   },
   "source": [
    "## get target data from source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V0JmIYd-5JQl",
    "outputId": "294393b2-d833-4c1a-bec2-f9c16ecdbae5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1148194/2358642364.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dF['bias_text'] = dF['bias_text'].str.capitalize()\n"
     ]
    }
   ],
   "source": [
    "input_file_dir = 'finalMergedData.json'\n",
    "rawDf = pd.read_json(input_file_dir)\n",
    "dF = rawDf[[\"source\",\"content\",\"bias_text\"]]\n",
    "dF['bias_text'] = dF['bias_text'].str.capitalize()\n",
    "#df.head()\n",
    "news_source = dF['source'].unique()\n",
    "#print(news_source.shape)\n",
    "bias_label = dF['bias_text'].unique()\n",
    "#print(bias_label)\n",
    "#df.groupby('bias_text').describe()\n",
    "df_filtered = dF[dF['bias_text'] != \"Political news media bias rating: not rated\"]  \n",
    "df_filtered = df_filtered[df_filtered['bias_text'] != \"Mixed\"]\n",
    "df_filtered = df_filtered.drop_duplicates(subset=['content'], keep='first')\n",
    "df_filtered = df_filtered[(df_filtered['content'] != \"\")]\n",
    "#df_filtered.describe()\n",
    "#df_filtered.groupby('bias_text').describe()\n",
    "df_filtered = df_filtered[df_filtered['bias_text'] != \"Lean left\"]\n",
    "df_filtered = df_filtered[df_filtered['bias_text'] != \"Lean right\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_je8JCa6mr9j"
   },
   "source": [
    "## begin cleaning text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WZHB4s0YmuHb",
    "outputId": "0c9a8859-a27b-4707-e704-391d4835c2ab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/dkang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 242
    },
    "id": "KFQEXklAm0bQ",
    "outputId": "aeb5f6d6-31b1-4fe8-b1a4-7b3388fce410"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1148194/1015252412.py:14: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_filtered['content'] = df_filtered['content'].str.replace('\\d+', '')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>content</th>\n",
       "      <th>bias_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Townhall</td>\n",
       "      <td>twotoone margin respondents participated recen...</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Townhall</td>\n",
       "      <td>president trump loves tweet way directly reach...</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New York Times - News</td>\n",
       "      <td>secret service chief revelations could threate...</td>\n",
       "      <td>Left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NPR Online News</td>\n",
       "      <td>trump ousts embattled campaign managerenlarge ...</td>\n",
       "      <td>Center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BBC News</td>\n",
       "      <td>media playback unsupported device media captio...</td>\n",
       "      <td>Center</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  source                                            content  \\\n",
       "0               Townhall  twotoone margin respondents participated recen...   \n",
       "1               Townhall  president trump loves tweet way directly reach...   \n",
       "2  New York Times - News  secret service chief revelations could threate...   \n",
       "3        NPR Online News  trump ousts embattled campaign managerenlarge ...   \n",
       "4               BBC News  media playback unsupported device media captio...   \n",
       "\n",
       "  bias_text  \n",
       "0     Right  \n",
       "1     Right  \n",
       "2      Left  \n",
       "3    Center  \n",
       "4    Center  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
    "#    text = re.sub(r'\\W+', '', text) # this removes all special characters # r means raw string\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwords from text\n",
    "    return text\n",
    "df_filtered['content'] = df_filtered['content'].apply(clean_text)\n",
    "# remove all digits\n",
    "df_filtered['content'] = df_filtered['content'].str.replace('\\d+', '')\n",
    "\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "n2sxQjJJm7Ps"
   },
   "outputs": [],
   "source": [
    "def countNumWordsOfContent(text):\n",
    "  return len(text.split())\n",
    "\n",
    "# remove those articles with actual words less than 30\n",
    "df_filtered = df_filtered[df_filtered['content'].apply(countNumWordsOfContent) >= 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "BuUaU5slm__L",
    "outputId": "53b80333-0c6f-40f3-8ec0-57c608580b4b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">source</th>\n",
       "      <th colspan=\"4\" halign=\"left\">content</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bias_text</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Center</th>\n",
       "      <td>12472</td>\n",
       "      <td>279</td>\n",
       "      <td>NPR Online News</td>\n",
       "      <td>2012</td>\n",
       "      <td>12472</td>\n",
       "      <td>12472</td>\n",
       "      <td>trump ousts embattled campaign managerenlarge ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Left</th>\n",
       "      <td>14886</td>\n",
       "      <td>173</td>\n",
       "      <td>CNN (Web News)</td>\n",
       "      <td>2905</td>\n",
       "      <td>14886</td>\n",
       "      <td>14886</td>\n",
       "      <td>secret service chief revelations could threate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Right</th>\n",
       "      <td>15571</td>\n",
       "      <td>175</td>\n",
       "      <td>Washington Times</td>\n",
       "      <td>2884</td>\n",
       "      <td>15571</td>\n",
       "      <td>15571</td>\n",
       "      <td>twotoone margin respondents participated recen...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          source                                content         \\\n",
       "           count unique               top  freq   count unique   \n",
       "bias_text                                                        \n",
       "Center     12472    279   NPR Online News  2012   12472  12472   \n",
       "Left       14886    173    CNN (Web News)  2905   14886  14886   \n",
       "Right      15571    175  Washington Times  2884   15571  15571   \n",
       "\n",
       "                                                                   \n",
       "                                                         top freq  \n",
       "bias_text                                                          \n",
       "Center     trump ousts embattled campaign managerenlarge ...    1  \n",
       "Left       secret service chief revelations could threate...    1  \n",
       "Right      twotoone margin respondents participated recen...    1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered = df_filtered.drop_duplicates(subset=['content'], keep='first')\n",
    "df_filtered = df_filtered[(df_filtered['content'] != \"\")]\n",
    "df_filtered.groupby('bias_text').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTTanrMRmvet"
   },
   "source": [
    "## Split data -- removing media source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "PdBX3Jm_rhkC"
   },
   "outputs": [],
   "source": [
    "#news_source is ndarray\n",
    "def clean_media_source(text):\n",
    "    text = ' '.join(word for word in text.split() if word not in news_source) # remove media sources from text\n",
    "    return text\n",
    "df_filtered['content'] = df_filtered['content'].apply(clean_media_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xnuP0SdceMxc",
    "outputId": "d6366cef-91ba-4405-cc82-88ed79ff5760",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38636,) (38636, 3)\n",
      "(4293,) (4293, 3)\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(df_filtered['bias_text']).values\n",
    "X = df_filtered['content'].values\n",
    "# split data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZR-sdFTGfQwa"
   },
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xg0SZV1E2hlw",
    "outputId": "7e83288f-db7d-4beb-90ab-549c715b6e0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model selected           : https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
     ]
    }
   ],
   "source": [
    "bert_model_name = 'bert_en_uncased_L-12_H-768_A-12' \n",
    "\n",
    "map_name_to_handle = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1'\n",
    "}\n",
    "\n",
    "map_model_to_preprocess = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
    "}\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "2dZYRzEe2iyH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-05 17:12:14.735286: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-05 17:12:14.735350: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-05 17:12:14.735385: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-05 17:12:14.737267: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-05 17:12:14.737311: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-05 17:12:14.737348: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-12-05 17:12:14.737358: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-12-05 17:12:14.738015: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "vmeXE_yU2lcs"
   },
   "outputs": [],
   "source": [
    "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "FPru5xJe20sN"
   },
   "outputs": [],
   "source": [
    "def build_classifier_model():\n",
    "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "  encoder_inputs = preprocessing_layer(text_input)\n",
    "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "  outputs = encoder(encoder_inputs)\n",
    "  pooled_output = outputs['pooled_output']\n",
    "  lay = tf.keras.layers.Dense(64, activation='relu')(pooled_output)\n",
    "  lay = tf.keras.layers.Dropout(0.2)(lay)\n",
    "  net = tf.keras.layers.Dense(3, activation='softmax', name='classifier')(lay)\n",
    "  return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "bZgxGPRQ23pF"
   },
   "outputs": [],
   "source": [
    "classifier_model = build_classifier_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "WIYw9iaz2_oK"
   },
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "metrics = tf.metrics.CategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "IkQyOWWE3DSa"
   },
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 64\n",
    "steps_per_epoch = len(X_train)\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "init_lr = 3e-5\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6zz4-WBqTp-"
   },
   "source": [
    "## Train and test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ja6s9pFT3Fy8"
   },
   "outputs": [],
   "source": [
    "classifier_model.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IsyuS_j73JNw",
    "outputId": "08931421-920d-440c-fac5-867ecedc2e9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\n",
      "Epoch 1/5\n",
      "544/544 [==============================] - ETA: 0s - loss: 1.1694 - categorical_accuracy: 0.3526 "
     ]
    }
   ],
   "source": [
    "print(f'Training model with {tfhub_handle_encoder}')\n",
    "history = classifier_model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9RkiguuL3LY6"
   },
   "outputs": [],
   "source": [
    "# convert the history.history dict to a pandas DataFrame:     \n",
    "hist_df = pd.DataFrame(history.history) \n",
    "\n",
    "# save to csv:  \n",
    "hist_csv_file = 'history_BERT_experiment2.csv'\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    hist_df.to_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RjDf8jtJok06"
   },
   "outputs": [],
   "source": [
    "accr = classifier_model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
    "f = open(\"result_BERT_experiment2.txt\", \"w\")\n",
    "f.write('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
