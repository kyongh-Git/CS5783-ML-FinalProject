{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "S63kdwjZtTdk"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 18:58:04.890973: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-04 18:58:05.020311: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-04 18:58:05.020344: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-04 18:58:05.561818: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-04 18:58:05.561887: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-04 18:58:05.561892: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Bidirectional\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jyFvlsC3tXir"
   },
   "outputs": [],
   "source": [
    "input_file_dir = 'finalMergedData.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VPgYEk8ItaD3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_902836/275354780.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['bias_text'] = df['bias_text'].str.capitalize()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">source</th>\n",
       "      <th colspan=\"4\" halign=\"left\">content</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bias_text</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Center</th>\n",
       "      <td>12687</td>\n",
       "      <td>286</td>\n",
       "      <td>NPR Online News</td>\n",
       "      <td>2012</td>\n",
       "      <td>12687</td>\n",
       "      <td>12687</td>\n",
       "      <td>Trump Ousts Embattled Campaign Manager\\n\\nEnla...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Left</th>\n",
       "      <td>14937</td>\n",
       "      <td>173</td>\n",
       "      <td>CNN (Web News)</td>\n",
       "      <td>2905</td>\n",
       "      <td>14937</td>\n",
       "      <td>14937</td>\n",
       "      <td>For Secret Service Chief, Revelations Could Th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Right</th>\n",
       "      <td>15594</td>\n",
       "      <td>176</td>\n",
       "      <td>Washington Times</td>\n",
       "      <td>2884</td>\n",
       "      <td>15594</td>\n",
       "      <td>15594</td>\n",
       "      <td>By a two-to-one margin, respondents who partic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          source                                content         \\\n",
       "           count unique               top  freq   count unique   \n",
       "bias_text                                                        \n",
       "Center     12687    286   NPR Online News  2012   12687  12687   \n",
       "Left       14937    173    CNN (Web News)  2905   14937  14937   \n",
       "Right      15594    176  Washington Times  2884   15594  15594   \n",
       "\n",
       "                                                                   \n",
       "                                                         top freq  \n",
       "bias_text                                                          \n",
       "Center     Trump Ousts Embattled Campaign Manager\\n\\nEnla...    1  \n",
       "Left       For Secret Service Chief, Revelations Could Th...    1  \n",
       "Right      By a two-to-one margin, respondents who partic...    1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawDf = pd.read_json(input_file_dir)\n",
    "df = rawDf[[\"source\",\"content\",\"bias_text\"]]\n",
    "df['bias_text'] = df['bias_text'].str.capitalize()\n",
    "news_source = df['source'].unique()\n",
    "bias_label = df['bias_text'].unique()\n",
    "df_filtered = df[df['bias_text'] != \"Political news media bias rating: not rated\"]  \n",
    "df_filtered = df_filtered[df_filtered['bias_text'] != \"Mixed\"]\n",
    "df_filtered = df_filtered.drop_duplicates(subset=['content'], keep='first')\n",
    "df_filtered = df_filtered[(df_filtered['content'] != \"\")]\n",
    "df_filtered = df_filtered[df_filtered['bias_text'] != \"Lean left\"]\n",
    "df_filtered = df_filtered[df_filtered['bias_text'] != \"Lean right\"]\n",
    "df_filtered.groupby('bias_text').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "p_ca4QuJtq1R"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/dkang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1CY40QQrttkV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_902836/2904369293.py:14: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_filtered['content'] = df_filtered['content'].str.replace('\\d+', '')\n"
     ]
    }
   ],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
    "#    text = re.sub(r'\\W+', '', text) # this removes all special characters # r means raw string\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwords from text\n",
    "    return text\n",
    "df_filtered['content'] = df_filtered['content'].apply(clean_text)\n",
    "# remove all digits\n",
    "df_filtered['content'] = df_filtered['content'].str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "PD2ezUeyty8k"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">source</th>\n",
       "      <th colspan=\"4\" halign=\"left\">content</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bias_text</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Center</th>\n",
       "      <td>12472</td>\n",
       "      <td>279</td>\n",
       "      <td>NPR Online News</td>\n",
       "      <td>2012</td>\n",
       "      <td>12472</td>\n",
       "      <td>12472</td>\n",
       "      <td>trump ousts embattled campaign managerenlarge ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Left</th>\n",
       "      <td>14886</td>\n",
       "      <td>173</td>\n",
       "      <td>CNN (Web News)</td>\n",
       "      <td>2905</td>\n",
       "      <td>14886</td>\n",
       "      <td>14886</td>\n",
       "      <td>secret service chief revelations could threate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Right</th>\n",
       "      <td>15571</td>\n",
       "      <td>175</td>\n",
       "      <td>Washington Times</td>\n",
       "      <td>2884</td>\n",
       "      <td>15571</td>\n",
       "      <td>15571</td>\n",
       "      <td>twotoone margin respondents participated recen...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          source                                content         \\\n",
       "           count unique               top  freq   count unique   \n",
       "bias_text                                                        \n",
       "Center     12472    279   NPR Online News  2012   12472  12472   \n",
       "Left       14886    173    CNN (Web News)  2905   14886  14886   \n",
       "Right      15571    175  Washington Times  2884   15571  15571   \n",
       "\n",
       "                                                                   \n",
       "                                                         top freq  \n",
       "bias_text                                                          \n",
       "Center     trump ousts embattled campaign managerenlarge ...    1  \n",
       "Left       secret service chief revelations could threate...    1  \n",
       "Right      twotoone margin respondents participated recen...    1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def countNumWordsOfContent(text):\n",
    "  return len(text.split())\n",
    "\n",
    "# remove those articles with actual words less than 30\n",
    "df_filtered = df_filtered[df_filtered['content'].apply(countNumWordsOfContent) >= 50]\n",
    "\n",
    "df_filtered = df_filtered.drop_duplicates(subset=['content'], keep='first')\n",
    "df_filtered = df_filtered[(df_filtered['content'] != \"\")]\n",
    "df_filtered.groupby('bias_text').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ObCgbPjdt-Os"
   },
   "outputs": [],
   "source": [
    "#news_source is ndarray\n",
    "def clean_media_source(text):\n",
    "    text = ' '.join(word for word in text.split() if word not in news_source) # remove media sources from text\n",
    "    return text\n",
    "df_filtered['content'] = df_filtered['content'].apply(clean_media_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "8ITOxrCUuA-e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 687488 unique tokens.\n",
      "Shape of data tensor: (42929, 8800)\n",
      "Shape of label tensor: (42929, 3)\n",
      "(38636, 8800) (38636, 3)\n",
      "(4293, 8800) (4293, 3)\n"
     ]
    }
   ],
   "source": [
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 689434\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 8800\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(df_filtered['content'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "X = tokenizer.texts_to_sequences(df_filtered['content'].values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)\n",
    "# convert categorical labels to numbers\n",
    "Y = pd.get_dummies(df_filtered['bias_text']).values\n",
    "print('Shape of label tensor:', Y.shape)\n",
    "# split data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4fJcWr-3uBuI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:03:45.086880: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-04 19:03:45.086947: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-04 19:03:45.086983: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-04 19:03:45.091307: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-04 19:03:45.091352: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-04 19:03:45.091390: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-12-04 19:03:45.091405: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-12-04 19:03:45.091919: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " 30/544 [>.............................] - ETA: 2:18:10 - loss: 1.0950 - accuracy: 0.4401"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the history.history dict to a pandas DataFrame:     \n",
    "hist_df = pd.DataFrame(history.history) \n",
    "\n",
    "# save to csv:  \n",
    "hist_csv_file = 'history_BLSTM_experiment2.csv'\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    hist_df.to_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2jQ4QkwvuGH5"
   },
   "outputs": [],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
    "f = open(\"result_BLSTM_experiment2.txt\", \"w\")\n",
    "f.write('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hDuZwuGCuIQW"
   },
   "outputs": [],
   "source": [
    "# plt.title('Loss')\n",
    "# plt.plot(history.history['loss'], label='train')\n",
    "# plt.plot(history.history['val_loss'], label='test')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "# plt.savefig('loss_LSTM_experiment2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3B9jXIzeuKfo"
   },
   "outputs": [],
   "source": [
    "# plt.title('Accuracy')\n",
    "# plt.plot(history.history['acc'], label='train')\n",
    "# plt.plot(history.history['val_acc'], label='test')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "# plt.savefig('accurate_LSTM_experiment2.png')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
